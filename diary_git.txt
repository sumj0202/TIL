6월 18일(수)
오늘의 나는 무엇을 잘 했는지 ?
토큰화와 임베딩 그리고 단어사전 생성에 대해서 솔직히 절차는 알겠는데,
임베딩 행렬을 생성한다는게 크게 의미가 와닿지 않았는데, 이번에 RNN(순환신경망) 개념을 정리하면서 고유의 값을 정하고 그 값에 따라 연산되어 예측한다는 것을 완전히는 아니어도
조금은 이해되었고, 헷갈리던 텍스트 데이터 분석, 전처리와 임베딩 절차에 대해서는 수업자료를 참조 하지않아도 해낼 수 있게 되었다.

나만의 실습 팁
수업중에 만든 실습 파일을 열고 큰 목록을 그대로 받아적은 뒤,
절차에 맞춰 써야할 함수들을 적는다.(도구 준비)
함수들을 활용하여 코드를 채워 넣는다.(도구 활용)
실습 파일과 비교하여 자신이 모르는 부분을 채워넣는다.( 보충)
그리고 이해가 가지않는 매개변수들을 CHATGPT를 통하여 이해하고 참조한다.

6월19일
오늘 무엇을 배웠고, 기억해야 하는지?
RNN(순환신경망)모델 실습을 주로 진행했는데,
항상 여러 모델 그리고 함수들을 배울 때 가장 힘든게, 매개변수 설정인것같다.
이 함수가 가진 매개변수가 많을 땐 너무 많고, 매개변수의 의미를 두루뭉실하게 알고있으니
조금 벅찰 때도 있다.
다시 실습했던 내용을 복습할 때 마다 기억해내기가 힘들다.
함수마다 매개변수를 기억할 좋은 방법이 있는지 고민하고 여러가지 방법을 시도해 볼 예정이다.
그리고 함수 임폴트 하는것도 점점 많아져서 기억하는게 힘듬...

RNN신경망 진행은
embedding layer 설정, RNN layer 설정, dense layer 설정 -> compile -> fit (학습)
 순서는 전에 했던 CNN모델이랑 같아서 쉬웠다.
그리고 매개변수들도 비슷해서 좀 더 이해하기 쉬웠다.
하지만 보지않은 채로 적용하는건 아직 힘들다.

그리고 특이했던 점은
fit(학습)하기 전에 early stop 와 checkpoint를 설정하는 것이었다.
여기서 early stop은 반복학습을 진행하면서, 더 학습을 진행해도 성능이 좋아지지 않는다면
설정한 epochs보다 조기에 종료되는것이다.
물론 바로 종료는 아니고, patience 변수에 넣은 값까지는 기다려줌.

checkpoint는 게임으로 치면 세이브지점이다.
학습 하는 동안 가장 최적의 성능일 때를 keras파일로 저장한다.

역시 근데, 이해보다 암기가 더 중요한 듯 싶어 개념보다는 실습에 대한 반복의 비중을 더 늘려야겠다.
사람은 기억할 때 상황이 있고, 손으로 행동해야 더 잘기억한다고 하니 말이다.