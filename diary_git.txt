6월 18일(수)
오늘의 나는 무엇을 잘 했는지 ?
토큰화와 임베딩 그리고 단어사전 생성에 대해서 솔직히 절차는 알겠는데,
임베딩 행렬을 생성한다는게 크게 의미가 와닿지 않았는데, 이번에 RNN(순환신경망) 개념을 정리하면서 고유의 값을 정하고 그 값에 따라 연산되어 예측한다는 것을 완전히는 아니어도
조금은 이해되었고, 헷갈리던 텍스트 데이터 분석, 전처리와 임베딩 절차에 대해서는 수업자료를 참조 하지않아도 해낼 수 있게 되었다.

나만의 실습 팁
수업중에 만든 실습 파일을 열고 큰 목록을 그대로 받아적은 뒤,
절차에 맞춰 써야할 함수들을 적는다.(도구 준비)
함수들을 활용하여 코드를 채워 넣는다.(도구 활용)
실습 파일과 비교하여 자신이 모르는 부분을 채워넣는다.( 보충)
그리고 이해가 가지않는 매개변수들을 CHATGPT를 통하여 이해하고 참조한다.

6월19일 (목)
오늘 무엇을 배웠고, 기억해야 하는지?
RNN(순환신경망)모델 실습을 주로 진행했는데,
항상 여러 모델 그리고 함수들을 배울 때 가장 힘든게, 매개변수 설정인것같다.
이 함수가 가진 매개변수가 많을 땐 너무 많고, 매개변수의 의미를 두루뭉실하게 알고있으니
조금 벅찰 때도 있다.
다시 실습했던 내용을 복습할 때 마다 기억해내기가 힘들다.
함수마다 매개변수를 기억할 좋은 방법이 있는지 고민하고 여러가지 방법을 시도해 볼 예정이다.
그리고 함수 임폴트 하는것도 점점 많아져서 기억하는게 힘듬...

RNN신경망 진행은
embedding layer 설정, RNN layer 설정, dense layer 설정 -> compile -> fit (학습)
 순서는 전에 했던 CNN모델이랑 같아서 쉬웠다.
그리고 매개변수들도 비슷해서 좀 더 이해하기 쉬웠다.
하지만 보지않은 채로 적용하는건 아직 힘들다.

그리고 특이했던 점은
fit(학습)하기 전에 early stop 와 checkpoint를 설정하는 것이었다.
여기서 early stop은 반복학습을 진행하면서, 더 학습을 진행해도 성능이 좋아지지 않는다면
설정한 epochs보다 조기에 종료되는것이다.
물론 바로 종료는 아니고, patience 변수에 넣은 값까지는 기다려줌.

checkpoint는 게임으로 치면 세이브지점이다.
학습 하는 동안 가장 최적의 성능일 때를 keras파일로 저장한다.

역시 근데, 이해보다 암기가 더 중요한 듯 싶어 개념보다는 실습에 대한 반복의 비중을 더 늘려야겠다.
사람은 기억할 때 상황이 있고, 손으로 행동해야 더 잘기억한다고 하니 말이다.

6월 20일 (금)
오늘은 LSTM모델을 활용해서 미국 금리 예측 실습을 했다.
전부다 하지않았는데, 모델 만들고 LAYER 추가 까지는 함.

기억에 남는 건 시계열 데이터이고, 예측이니까 학습용 평가용 데이터를 나눌 때
전에 썼던 train_test_split이 아니라 직접 수동으로 학습용으로 과거(80%) 최근(20%)로
직접 나누었다.
train_size = int(len(X_data)*0.8) -> 이런식으로 계산해서 분할함.

그다음에 sequentical() 를 이용하여 모델을 만들고 -> 껍데기
그 안에 LSTM layer를 두개 설정했다.
이 때 units 과 input_shape를 설정하는데
unit은 뉴런의 수(패턴을 몇개로 학습할 것인지?) input_shape는 (기간(일 수), 정보의 개수)라고 생각하면 편했다.

LSTM layer를 2개 만들구, dense layer를 만들어 평탄화 까지했다.
units= 1로 설정했음.

이게 오늘 수업시간 실습에서 배우고 생각한 점이고

오늘 혼자 궁금해서 미국 테슬라의 주가 예측을 해보고싶어서
yahoo finance로 부터 데이터를 다운받았음.
그런데,,,,,, 생각보다 엄청막힘... 어떤 컬럼이 중복되는? 컬럼일지도 생각하고
의외로 전처리가 어려웠다...
주말 간에는 통계기반_자연어처리에 대해 복습해서 시험 준비하구
이 주가예측에 대해서 조금 더 해볼 생각이다. 어떤 col이 중요할 지 또한
배경지식이 중요한 것 같다.
ㅂㅂ..