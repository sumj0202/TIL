6월 23일 (월)
오늘은 미국 금리 예측 마무리 실습을 했다.
새로 알게된 파라미터랑 명령어가 있음.
먼저 MinMaxScaler 함수에서 scaler.inverse_transform()을 하면 단어 뜻그대로 스케일 하기전
원상태로 복구된다.

그리고 plt.legend() -> 그래프의 선의 이름을 알려줌. 참조?
plt.gird(True) -> 이건 원래 false가 기본값이라 true하면 그래프에 격자생김.

마지막에 미래예측 하는법은, fit으로 학습되고 정답 데이터에서 predict로 평가한 모델을 준비
model.predict(input_data)
-> input_data는 원본 데이터에서 학습시킨 데이터의 길이만큼해서 변수 저장
-> 거기에 reshape로 형태를 (1,sequence_length,1) -> 학습된 데이터와 똑같은 형태로만듬

이렇게 만든 input_data를 model.predict()에 넣고 돌리면 됨
그리고 inverse_transform으로 sclae되기 전 값으로 복원시키면 끝~

++ 오후에는 드디어 transformer의 논리 구조 ? 역사 ? 를 배움.
강사님 말씀으로는 꽤나 중요한 기점의 논문이라고 한다.
self-attension이라는 것인데,
쉽게 풀어서 생각해보면.. RNN같은 애들은 한 단어 한 단어씩 순서대로 학습시켜서
시간도 오래걸리고, 문장이 길거나 대규 데이터와 모델이면 다루기가 어려움..
입력 시퀀스의 순서에 맞춰 연산 해서 병렬 처리도 어려움

그래서 이 한계를 극복하는 RNN을 대체하는 어텐션 알고리즘을 통해서 시퀀스 데이터를 처리

그 중에 transformer라는 모델이 있는데
구조는 인코더 와 디코더로 구별됨
word2vec의 문맥을 반영하여 임베딩 벡터를 만드는 구조는 비슷 
하지만 이 모델은 모든 문맥을 반영 시켜서 벡터를 만듬!

self-attension
과정 1 : 입력 문장 -> 토큰화(정수 인코딩) + 단어 임베딩 + positional encoding
이 단어 임베딩 과 positional encoding(순서를 기억하는 벡터라고 생각하면 쉬움)
--> 얘네 두개를 합쳐서 계속 학습(가중치 부여)

과정 2 : 각 단어 임베딩 벡터 -> 쿼리(query), 키(key), 값(value) 벡터 생성
이 세 벡터간의 연산을 통해서 문맥적 관계성을 추출하는 과정

솔직히 두루뭉술하게(대략적)으로는 이해가 간다. 하지만 깊고 구체적으로 생각할수록
머리가 아파온다.... 물론 이게 어떻게 계산하고 되는지는 코딩에 아무쓸모없지만,
계속 생각하면 머리아프다.. 그래도 논리구조가 어떤식으로 진행되는 지는 알것같다.
각 단어의 벡터에 문백 반영할만한 3개의 벡터로 문맥적 관계를 표현해서 학습? 이런거인듯