머신러닝 절차 공부 06-03

1. 데이터 불러오기
 # 경고 무시 설정
import warnings
warnings.filterwarnings('ignore')

# 필요한 라이브러리 임폴트
import pandas as pd

# 파일 경로 설정
file_path = '파일 경로'

# 데이터 프레임 생성
df = pd.read_csv(file_path)

2. 데이터 전처리
 2-1 불필요한 컬럼 제거
   ex) Unnamed: 0 컬럼 같은 것들.. 혹은 데이터 분석에 필요없는 columns
 2-2 누락데이터 확인
   num_null = df.isnull().sum()
   print(num_null)
 --> 누락 데이터 확인
   한 컬럼에 누락데이터가 너무 많을 경우 -> 과감히 해당 컬럼 삭제
    --> df.drop(columns=['컬럼이름'])으로 제거

 --> 누락 데이터 대체
       df.loc[:,'특정컬럼명'].fillna(대체할 값)
--> 누락 데이터가 있는 행 제거
       df.dropna(subset = [컬럼1,컬럼2,컬럼3], ignore_index=True)

 2-3  데이터 분석을 위해, 문자열데이터를 숫자로 변환
       (숫자)(공백)(단위)일 경우 split함수를 이용하여 숫자만 남게만든다.
       그 후 df.astype({컬럼명:바꿀타입}) 사용
           --> float, int로 데이터 타입을 변경한다.

2-4 이상치 제거 (선택사항)
q1= df.loc[:,column].contile(q=0.25)
q3= df.loc[:,column].contile(q=0.75)

iqr = q3-q1

min = q1- (1.5*iqr)
max = q3 + (1.5*iqr)

비교 연산자 + 논리 연산자 사용하여 조건설정
condition = (df.loc[:,column] < min) | (df.loc[:,column] > max)
outliner_index = df.loc[condition,columns].index
로 이상치 있는 컬럼 확인

loc함수로 이상치 있는 컬럼으로만 df 생성
그리고 이상치를 제거한 뒤
concat 함수로 기존 데이터랑 병합
pd.concat([column1,column2, ... , column5], axis=1, join='inner)
axis=1 --> y축방향으로 병합(컬럼병합이라고 생각)
inner는 교집합

병합후 reset_index(drop=True) 함수 사용하여 인덱스번호 정렬

2-5 label encoding

# 필요함수 임폴트
import matplotlib.pyplot as plt

# label encoding이 필요한 컬럼 시각화
df.loc[:,column].value_counts().plot(kind='bar')
plt.show()

--> 가짓수가 적으므로, replace함수를 이용하여 숫자로 표현
replace( {종류1 : 0, 종류2 : 1, 종류3 : 2, A : 0, B : 1} )

전처리한 데이터 저장
file_path = '파일경로/생성파일명'
df.to_csv(파일경로)

3-1 학습용 데이터와 평가용 데이터 생성

# 필요한 함수 임폴트
from sklearn.model_selection import train_test_split

 X_data 생성 
 정답 column을 drop함수로 제거

y_data 생성 ( 정답 데이터 )
 정답 column을 loc함수를 사용하여 인덱싱

X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size = 0.2, random_state = 0)
test_size : 평가용 데이터의 비율
random_state : 재현성

4-1 회귀모델을 이용하여 예측
필요함수 임폴트
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import GridSearchCV
import numpy as np

4-2 최적화할 기본 모델 생성
dt = DecisionTreeRegressor(random_state=0)

4-3 GridSearchCV를 통하여 최적화 매개변수와 탐색 조건 설정
depth = np.arange(5,21)
params = {'max_depth' : depth}

grid_dt = GridSearchCV( estimator = dt, param_grid = params, scoring = 'neg_root_squared_error', cv= 10)

4-4 학습 및 평가
grid_dt.fit(X_train, y_train)

5-1 best 모델 추출
best_dt = grid_dt.best_estimator_

5-2 평가용 데이터 예측
pred_dt = best_dt.predict(X_test)

5-3 모델 평가
# 필요함수 임폴트
from sklearn.metrics import root_mean_squared_error as rmse

rmse = rmse(y_test, pred_dt)

5-4 결과 확인
print(rmse)

--> rmse는 오차율 , 정답의 값 그리고 상황에 따라 오차율이 큰지 작은지 확인할 수 있음.

6-1 SHAP 분석
SHAP --> 정답예측에 대한 각 컬럼별 관계를 알기 위한 도구

1. 필요 라이브러리 임폴트
import shap
shap.initjs() --> shap 시각화를 위해 javascript 라이브러리 설정.

2. 적합한 설명모델 생성
explainer = shap.TreeExplainer(best_dt1)

3. shap value 계산
shap_values = explainer(X_train)

4. 각 컬럼별 중요도와 영향력 시각화
shap.summary_plot(shap_values)

5. 개별 데이터에 대한 컬럼별 중요도와 영향력 시각화
shap.plots.waterfall(shap_values[0])
--> 개별 데이터중 첫번째 데이터의 자료 시각화

6월5일 머신러닝 복습 -----------------------------------------
lgbm  = LGBMRegressor(random_state=0,
                      bagging_seed=0,
                      feature_fraction_seed=0,
                      data_random_seed=0,
                      drop_seed=0,
                      verbose=-1)
lgbm hyperparameter 저장.

lgbm 과 xgboost

lgbm은 알아서 가지치기를 함으로써, 학습할 때 과적합을 방지해준다.
그리고 속도도 2~3배 정도 xgboost 보다 빠르다.

**Logistic Regression**
회귀를 이용한 분류이다.
분류를 축에 수직인 선으로 하는게 아니라,
회귀를 통해 만든 직선을 경계로 분류하는 것이다.
이 직선을 결정 경계라고한다.

임폴트 함수
from sklearn.linear_model import LogisticRegression

logistic = LogisticRegression(hyperparameter)

모델 학습..
fit(X_train, y_train)
예측..
predict

